{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52e4735d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VisualBertModel(\n",
      "  (embeddings): VisualBertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=1)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (visual_token_type_embeddings): Embedding(2, 768)\n",
      "    (visual_position_embeddings): Embedding(512, 768)\n",
      "    (visual_projection): Linear(in_features=2048, out_features=768, bias=True)\n",
      "  )\n",
      "  (encoder): VisualBertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0): VisualBertLayer(\n",
      "        (attention): VisualBertAttention(\n",
      "          (self): VisualBertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): VisualBertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): VisualBertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): VisualBertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): VisualBertLayer(\n",
      "        (attention): VisualBertAttention(\n",
      "          (self): VisualBertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): VisualBertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): VisualBertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): VisualBertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): VisualBertLayer(\n",
      "        (attention): VisualBertAttention(\n",
      "          (self): VisualBertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): VisualBertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): VisualBertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): VisualBertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): VisualBertLayer(\n",
      "        (attention): VisualBertAttention(\n",
      "          (self): VisualBertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): VisualBertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): VisualBertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): VisualBertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): VisualBertLayer(\n",
      "        (attention): VisualBertAttention(\n",
      "          (self): VisualBertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): VisualBertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): VisualBertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): VisualBertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): VisualBertLayer(\n",
      "        (attention): VisualBertAttention(\n",
      "          (self): VisualBertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): VisualBertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): VisualBertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): VisualBertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (6): VisualBertLayer(\n",
      "        (attention): VisualBertAttention(\n",
      "          (self): VisualBertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): VisualBertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): VisualBertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): VisualBertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (7): VisualBertLayer(\n",
      "        (attention): VisualBertAttention(\n",
      "          (self): VisualBertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): VisualBertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): VisualBertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): VisualBertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (8): VisualBertLayer(\n",
      "        (attention): VisualBertAttention(\n",
      "          (self): VisualBertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): VisualBertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): VisualBertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): VisualBertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (9): VisualBertLayer(\n",
      "        (attention): VisualBertAttention(\n",
      "          (self): VisualBertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): VisualBertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): VisualBertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): VisualBertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (10): VisualBertLayer(\n",
      "        (attention): VisualBertAttention(\n",
      "          (self): VisualBertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): VisualBertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): VisualBertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): VisualBertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (11): VisualBertLayer(\n",
      "        (attention): VisualBertAttention(\n",
      "          (self): VisualBertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): VisualBertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): VisualBertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): VisualBertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): VisualBertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n",
      "{'input_ids': tensor([[  101,  2054,  2003,  1996, 28829,  8153,  1029,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]]), 'visual_embeds': tensor([[[0.3791, 0.1347, 0.8402,  ..., 0.2518, 0.5933, 0.5002]]]), 'visual_token_type_ids': tensor([[1]]), 'visual_attention_mask': tensor([[1.]])}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 30522])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from transformers import VisualBertForQuestionAnswering\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "encoder = VisualBertForQuestionAnswering.from_pretrained('uclanlp/visualbert-vqa')\n",
    "encoder = encoder.visual_bert\n",
    "\n",
    "print(encoder)\n",
    "\n",
    "inputs = tokenizer(\"what is the defective tissue?\", return_tensors=\"pt\", padding=\"max_length\", max_length=15)\n",
    "labels = tokenizer(\"it is kidney\", return_tensors=\"pt\").input_ids\n",
    "\n",
    "visual_embeds = torch.rand(1, 1, 2048)\n",
    "visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)\n",
    "visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)\n",
    "\n",
    "# print(visual_embeds.shape)\n",
    "# print(visual_token_type_ids.shape)\n",
    "# print(visual_token_type_ids.shape)\n",
    "\n",
    "inputs.update({\n",
    "\"visual_embeds\": visual_embeds,\n",
    "\"visual_token_type_ids\": visual_token_type_ids,\n",
    "\"visual_attention_mask\": visual_attention_mask\n",
    "})\n",
    "\n",
    "print(inputs)\n",
    "# inputs[\"attention_mask\"] = None\n",
    "\n",
    "outputs = encoder(**inputs)\n",
    "\n",
    "from transformers.models.bert.modeling_bert import BertLMPredictionHead\n",
    "predict = BertLMPredictionHead(encoder.config)\n",
    "outputs = predict(outputs['last_hidden_state'])\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcf1c3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "class SurgicalSentenceVQADataset(Dataset):\n",
    "    def __init__(self, seq, folder_head, folder_tail, transform=None):\n",
    "        \n",
    "        self.transform = transform\n",
    "        \n",
    "        # files, question and answers\n",
    "        filenames = []\n",
    "        for curr_seq in seq: filenames = filenames + glob.glob(folder_head + str(curr_seq) + folder_tail)\n",
    "        self.vqas = []\n",
    "        for file in filenames:\n",
    "            file_data = open(file, \"r\")\n",
    "            lines = [line.strip(\"\\n\") for line in file_data if line != \"\\n\"]\n",
    "            file_data.close()\n",
    "            for line in lines: self.vqas.append([file, line])\n",
    "        print('Total files: %d | Total question: %.d' %(len(filenames), len(self.vqas)))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.vqas)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # img\n",
    "        loc = self.vqas[idx][0].split('/')\n",
    "        img_loc = os.path.join(loc[0],loc[1],loc[2], 'left_frames',loc[-1].split('_')[0]+'.png')\n",
    "        img = Image.open(img_loc)\n",
    "        if self.transform: img = self.transform(img)\n",
    "            \n",
    "        # question and answer\n",
    "        question = self.vqas[idx][1].split('|')[0]\n",
    "        label = self.vqas[idx][1].split('|')[1]\n",
    "\n",
    "        return img, question, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85231c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from transformers import VisualBertForQuestionAnswering\n",
    "from transformers.models.bert.modeling_bert import BertLMPredictionHead\n",
    "\n",
    "class Surgical_VQA(nn.Module):\n",
    "    def __init__(self, num_classes=12):\n",
    "        super(Surgical_VQA, self).__init__()\n",
    "        \n",
    "        # visual feature extraction\n",
    "        self.img_feature_extractor = models.resnet50(pretrained=True)\n",
    "        new_fc = nn.Sequential(*list(self.img_feature_extractor.fc.children())[:-1])\n",
    "        self.img_feature_extractor.fc = new_fc\n",
    "        \n",
    "        # visual + caption feature extractor\n",
    "        vis_transformer = VisualBertForQuestionAnswering.from_pretrained('uclanlp/visualbert-vqa')\n",
    "        self.transformer_encoder = vis_transformer.visual_bert\n",
    "\n",
    "        #classifier\n",
    "        self.predict_sentence = BertLMPredictionHead(self.transformer_encoder.config)\n",
    "\n",
    "    def forward(self, img, inputs):\n",
    "        \n",
    "        visual_embeds = self.img_feature_extractor(img)\n",
    "        visual_embeds = torch.unsqueeze(visual_embeds, 1)\n",
    "        visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long).cuda()\n",
    "        visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float).cuda()\n",
    "\n",
    "        inputs.update({\n",
    "        \"visual_embeds\": visual_embeds,\n",
    "        \"visual_token_type_ids\": visual_token_type_ids,\n",
    "        \"visual_attention_mask\": visual_attention_mask\n",
    "        })\n",
    "        \n",
    "        inputs['input_ids'] = inputs['input_ids'].cuda()\n",
    "        inputs['token_type_ids'] = inputs['token_type_ids'].cuda()\n",
    "        inputs['attention_mask'] = inputs['attention_mask'].cuda()\n",
    "        inputs['visual_token_type_ids'] = inputs['visual_token_type_ids'].cuda()\n",
    "        inputs['visual_attention_mask'] = inputs['visual_attention_mask'].cuda()\n",
    "        \n",
    "        out = self.transformer_encoder(**inputs)\n",
    "        out = self.predict_sentence(out['last_hidden_state'])\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acab0ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "def calc_acc(y_true, y_pred):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    return acc\n",
    "\n",
    "def calc_classwise_acc(y_true, y_pred):\n",
    "    matrix = confusion_matrix(y_true, y_pred)\n",
    "    classwise_acc = matrix.diagonal()/matrix.sum(axis=1)\n",
    "    return classwise_acc\n",
    "\n",
    "def calc_map(y_true, y_scores):\n",
    "    mAP = average_precision_score(y_true, y_scores,average=None)\n",
    "    return mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac2f7bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def test_model(epoch, model, valid_dataloader, tokenizer):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0.0    \n",
    "    label_true = None\n",
    "    label_pred = None\n",
    "    label_score = None\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (imgs, q, l) in enumerate(valid_dataloader, 0):\n",
    "            questions = []\n",
    "            labels = []\n",
    "            for question in q: questions.append(question)\n",
    "            for label in l: labels.append(label)\n",
    "            \n",
    "            inputs = tokenizer(questions, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=15)\n",
    "            GT_labels = tokenizer(labels, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=16).input_ids\n",
    "            \n",
    "            imgs, GT_labels = imgs.cuda(), GT_labels.cuda()\n",
    "            \n",
    "            outputs = model(imgs, inputs)\n",
    "\n",
    "            loss = criterion(torch.flatten(outputs, start_dim=0, end_dim = 1), torch.flatten(GT_labels))\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "            scores, predicted = torch.max(F.softmax(torch.flatten(outputs, start_dim=0, end_dim = 1), dim=1).data, 1)    \n",
    "            label_true = torch.flatten(GT_labels).data.cpu() if label_true == None else torch.cat((label_true, torch.flatten(GT_labels).data.cpu()), 0)\n",
    "            label_pred = predicted.data.cpu() if label_pred == None else torch.cat((label_pred, predicted.data.cpu()), 0)\n",
    "            label_score = scores.data.cpu() if label_score == None else torch.cat((label_score, scores.data.cpu()), 0)\n",
    "\n",
    "            \n",
    "    acc, c_acc, mAP = calc_acc(label_true, label_pred), calc_classwise_acc(label_true, label_pred), 0.0#calc_map(label_true, label_score)\n",
    "\n",
    "    print('Test: epoch: %d loss: %.6f | Acc: %.6f | mAP: %.6f' %(epoch, total_loss, acc, mAP))\n",
    "#     print(c_acc)\n",
    "    \n",
    "    return (acc, c_acc, mAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "867cc12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "def train_model(epoch, model, train_dataloader, lr, tokenizer):  # train model\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0.0    \n",
    "    label_true = None\n",
    "    label_pred = None\n",
    "    label_score = None\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr = lr, weight_decay = 0)\n",
    "    \n",
    "    for i, (imgs, q, l) in enumerate(train_dataloader,0):\n",
    "        questions = []\n",
    "        labels = []\n",
    "        for question in q: questions.append(question)\n",
    "        for label in l: labels.append(label)\n",
    "            \n",
    "        inputs = tokenizer(questions, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=15)\n",
    "        GT_labels = tokenizer(labels, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=16).input_ids\n",
    "            \n",
    "        imgs, GT_labels = imgs.cuda(), GT_labels.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "        outputs = model(imgs, inputs)\n",
    "        \n",
    "#         print(outputs.shape, GT_labels.shape)\n",
    "        # zero the parameter gradients\n",
    "\n",
    "        loss = criterion(torch.flatten(outputs, start_dim=0, end_dim = 1), torch.flatten(GT_labels))\n",
    "#         loss = criterion(outputs, GT_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        scores, predicted = torch.max(F.softmax(torch.flatten(outputs, start_dim=0, end_dim = 1), dim=1).data, 1)    \n",
    "        label_true = torch.flatten(GT_labels).data.cpu() if label_true == None else torch.cat((label_true, torch.flatten(GT_labels).data.cpu()), 0)\n",
    "        label_pred = predicted.data.cpu() if label_pred == None else torch.cat((label_pred, predicted.data.cpu()), 0)\n",
    "        label_score = scores.data.cpu() if label_score == None else torch.cat((label_score, scores.data.cpu()), 0)\n",
    "\n",
    "    \n",
    "    # loss and acc\n",
    "    acc, c_acc, mAP = calc_acc(label_true, label_pred), calc_classwise_acc(label_true, label_pred), 0.0#calc_map(label_true, label_score)\n",
    "\n",
    "    print('Train: epoch: %d loss: %.6f | Acc: %.6f | mAP: %.6f' %(epoch, total_loss, acc, mAP))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd0b5915",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 137 | Total question: 1046\n",
      "Total files: 447 | Total question: 3216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12517/2798414565.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
      "  classwise_acc = matrix.diagonal()/matrix.sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: epoch: 1 loss: 415.087739 | Acc: 0.221080 | mAP: 0.000000\n",
      "Test: epoch: 1 loss: 284.009027 | Acc: 0.401231 | mAP: 0.000000\n",
      "Best epoch: 1 | Best acc: 0.401231\n",
      "Train: epoch: 2 loss: 238.786792 | Acc: 0.593212 | mAP: 0.000000\n",
      "Test: epoch: 2 loss: 137.994969 | Acc: 0.795351 | mAP: 0.000000\n",
      "Best epoch: 2 | Best acc: 0.795351\n",
      "Train: epoch: 3 loss: 130.141966 | Acc: 0.795411 | mAP: 0.000000\n",
      "Test: epoch: 3 loss: 68.830898 | Acc: 0.868786 | mAP: 0.000000\n",
      "Best epoch: 3 | Best acc: 0.868786\n",
      "Train: epoch: 4 loss: 71.501628 | Acc: 0.873865 | mAP: 0.000000\n",
      "Test: epoch: 4 loss: 37.233835 | Acc: 0.904159 | mAP: 0.000000\n",
      "Best epoch: 4 | Best acc: 0.904159\n",
      "Train: epoch: 5 loss: 41.238166 | Acc: 0.913838 | mAP: 0.000000\n",
      "Test: epoch: 5 loss: 22.294852 | Acc: 0.937082 | mAP: 0.000000\n",
      "Best epoch: 5 | Best acc: 0.937082\n",
      "Train: epoch: 6 loss: 26.251845 | Acc: 0.935050 | mAP: 0.000000\n",
      "Test: epoch: 6 loss: 14.522917 | Acc: 0.945686 | mAP: 0.000000\n",
      "Best epoch: 6 | Best acc: 0.945686\n",
      "Train: epoch: 7 loss: 17.810757 | Acc: 0.944909 | mAP: 0.000000\n",
      "Test: epoch: 7 loss: 10.618656 | Acc: 0.954469 | mAP: 0.000000\n",
      "Best epoch: 7 | Best acc: 0.954469\n",
      "Train: epoch: 8 loss: 12.777423 | Acc: 0.953633 | mAP: 0.000000\n",
      "Test: epoch: 8 loss: 7.263006 | Acc: 0.968212 | mAP: 0.000000\n",
      "Best epoch: 8 | Best acc: 0.968212\n",
      "Train: epoch: 9 loss: 9.579567 | Acc: 0.961341 | mAP: 0.000000\n",
      "Test: epoch: 9 loss: 5.686784 | Acc: 0.968093 | mAP: 0.000000\n",
      "Best epoch: 8 | Best acc: 0.968212\n",
      "Train: epoch: 10 loss: 7.610549 | Acc: 0.966300 | mAP: 0.000000\n",
      "Test: epoch: 10 loss: 4.697757 | Acc: 0.975681 | mAP: 0.000000\n",
      "Best epoch: 10 | Best acc: 0.975681\n",
      "Train: epoch: 11 loss: 6.211554 | Acc: 0.970602 | mAP: 0.000000\n",
      "Test: epoch: 11 loss: 4.094610 | Acc: 0.976219 | mAP: 0.000000\n",
      "Best epoch: 11 | Best acc: 0.976219\n",
      "Train: epoch: 12 loss: 5.525218 | Acc: 0.970244 | mAP: 0.000000\n",
      "Test: epoch: 12 loss: 3.281927 | Acc: 0.979744 | mAP: 0.000000\n",
      "Best epoch: 12 | Best acc: 0.979744\n",
      "Train: epoch: 13 loss: 4.693258 | Acc: 0.972574 | mAP: 0.000000\n",
      "Test: epoch: 13 loss: 3.023175 | Acc: 0.978967 | mAP: 0.000000\n",
      "Best epoch: 12 | Best acc: 0.979744\n",
      "Train: epoch: 14 loss: 3.958618 | Acc: 0.975681 | mAP: 0.000000\n",
      "Test: epoch: 14 loss: 2.852854 | Acc: 0.980641 | mAP: 0.000000\n",
      "Best epoch: 14 | Best acc: 0.980641\n",
      "Train: epoch: 15 loss: 3.814610 | Acc: 0.976398 | mAP: 0.000000\n",
      "Test: epoch: 15 loss: 2.451013 | Acc: 0.983509 | mAP: 0.000000\n",
      "Best epoch: 15 | Best acc: 0.983509\n",
      "Train: epoch: 16 loss: 3.221803 | Acc: 0.978011 | mAP: 0.000000\n",
      "Test: epoch: 16 loss: 2.388085 | Acc: 0.983210 | mAP: 0.000000\n",
      "Best epoch: 15 | Best acc: 0.983509\n",
      "Train: epoch: 17 loss: 3.036431 | Acc: 0.980939 | mAP: 0.000000\n",
      "Test: epoch: 17 loss: 2.003812 | Acc: 0.988767 | mAP: 0.000000\n",
      "Best epoch: 17 | Best acc: 0.988767\n",
      "Train: epoch: 18 loss: 2.933700 | Acc: 0.982851 | mAP: 0.000000\n",
      "Test: epoch: 18 loss: 2.077349 | Acc: 0.984405 | mAP: 0.000000\n",
      "Best epoch: 17 | Best acc: 0.988767\n",
      "Train: epoch: 19 loss: 2.490796 | Acc: 0.984584 | mAP: 0.000000\n",
      "Test: epoch: 19 loss: 1.539696 | Acc: 0.991157 | mAP: 0.000000\n",
      "Best epoch: 19 | Best acc: 0.991157\n",
      "Train: epoch: 20 loss: 2.325570 | Acc: 0.985182 | mAP: 0.000000\n",
      "Test: epoch: 20 loss: 1.446304 | Acc: 0.991217 | mAP: 0.000000\n",
      "Best epoch: 20 | Best acc: 0.991217\n",
      "Train: epoch: 21 loss: 2.035717 | Acc: 0.986616 | mAP: 0.000000\n",
      "Test: epoch: 21 loss: 1.366191 | Acc: 0.991754 | mAP: 0.000000\n",
      "Best epoch: 21 | Best acc: 0.991754\n",
      "Train: epoch: 22 loss: 1.821600 | Acc: 0.988647 | mAP: 0.000000\n",
      "Test: epoch: 22 loss: 1.307293 | Acc: 0.991993 | mAP: 0.000000\n",
      "Best epoch: 22 | Best acc: 0.991993\n",
      "Train: epoch: 23 loss: 1.784257 | Acc: 0.988408 | mAP: 0.000000\n",
      "Test: epoch: 23 loss: 1.256897 | Acc: 0.992232 | mAP: 0.000000\n",
      "Best epoch: 23 | Best acc: 0.992232\n",
      "Train: epoch: 24 loss: 1.955786 | Acc: 0.986616 | mAP: 0.000000\n",
      "Test: epoch: 24 loss: 1.269800 | Acc: 0.992292 | mAP: 0.000000\n",
      "Best epoch: 24 | Best acc: 0.992292\n",
      "Train: epoch: 25 loss: 1.623270 | Acc: 0.989304 | mAP: 0.000000\n",
      "Test: epoch: 25 loss: 1.070855 | Acc: 0.992471 | mAP: 0.000000\n",
      "Best epoch: 25 | Best acc: 0.992471\n",
      "Train: epoch: 26 loss: 1.883847 | Acc: 0.989543 | mAP: 0.000000\n",
      "Test: epoch: 26 loss: 1.127003 | Acc: 0.991814 | mAP: 0.000000\n",
      "Best epoch: 25 | Best acc: 0.992471\n",
      "Train: epoch: 27 loss: 1.439792 | Acc: 0.990559 | mAP: 0.000000\n",
      "Test: epoch: 27 loss: 1.207469 | Acc: 0.992173 | mAP: 0.000000\n",
      "Best epoch: 25 | Best acc: 0.992471\n",
      "Train: epoch: 28 loss: 1.653564 | Acc: 0.989484 | mAP: 0.000000\n",
      "Test: epoch: 28 loss: 0.918405 | Acc: 0.992830 | mAP: 0.000000\n",
      "Best epoch: 28 | Best acc: 0.992830\n",
      "Train: epoch: 29 loss: 1.608002 | Acc: 0.989902 | mAP: 0.000000\n",
      "Test: epoch: 29 loss: 0.923278 | Acc: 0.992890 | mAP: 0.000000\n",
      "Best epoch: 29 | Best acc: 0.992890\n",
      "Train: epoch: 30 loss: 1.160918 | Acc: 0.991695 | mAP: 0.000000\n",
      "Test: epoch: 30 loss: 0.743267 | Acc: 0.995100 | mAP: 0.000000\n",
      "Best epoch: 30 | Best acc: 0.995100\n",
      "Train: epoch: 31 loss: 1.177820 | Acc: 0.992830 | mAP: 0.000000\n",
      "Test: epoch: 31 loss: 0.631958 | Acc: 0.995459 | mAP: 0.000000\n",
      "Best epoch: 31 | Best acc: 0.995459\n",
      "Train: epoch: 32 loss: 0.965148 | Acc: 0.993547 | mAP: 0.000000\n",
      "Test: epoch: 32 loss: 0.600301 | Acc: 0.995698 | mAP: 0.000000\n",
      "Best epoch: 32 | Best acc: 0.995698\n",
      "Train: epoch: 33 loss: 0.941466 | Acc: 0.993786 | mAP: 0.000000\n",
      "Test: epoch: 33 loss: 0.561077 | Acc: 0.995937 | mAP: 0.000000\n",
      "Best epoch: 33 | Best acc: 0.995937\n",
      "Train: epoch: 34 loss: 1.078675 | Acc: 0.994324 | mAP: 0.000000\n",
      "Test: epoch: 34 loss: 0.609655 | Acc: 0.995698 | mAP: 0.000000\n",
      "Best epoch: 33 | Best acc: 0.995937\n",
      "Train: epoch: 35 loss: 0.802050 | Acc: 0.994742 | mAP: 0.000000\n",
      "Test: epoch: 35 loss: 0.591618 | Acc: 0.995399 | mAP: 0.000000\n",
      "Best epoch: 33 | Best acc: 0.995937\n",
      "Train: epoch: 36 loss: 1.182648 | Acc: 0.992471 | mAP: 0.000000\n",
      "Test: epoch: 36 loss: 0.533536 | Acc: 0.995877 | mAP: 0.000000\n",
      "Best epoch: 33 | Best acc: 0.995937\n",
      "Train: epoch: 37 loss: 0.782774 | Acc: 0.994264 | mAP: 0.000000\n",
      "Test: epoch: 37 loss: 0.880715 | Acc: 0.994204 | mAP: 0.000000\n",
      "Best epoch: 33 | Best acc: 0.995937\n",
      "Train: epoch: 38 loss: 0.755405 | Acc: 0.994563 | mAP: 0.000000\n",
      "Test: epoch: 38 loss: 0.555208 | Acc: 0.995758 | mAP: 0.000000\n",
      "Best epoch: 33 | Best acc: 0.995937\n",
      "Train: epoch: 39 loss: 0.813452 | Acc: 0.994443 | mAP: 0.000000\n",
      "Test: epoch: 39 loss: 0.532089 | Acc: 0.995937 | mAP: 0.000000\n",
      "Best epoch: 39 | Best acc: 0.995937\n",
      "Train: epoch: 40 loss: 0.856685 | Acc: 0.994264 | mAP: 0.000000\n",
      "Test: epoch: 40 loss: 0.432735 | Acc: 0.996773 | mAP: 0.000000\n",
      "Best epoch: 40 | Best acc: 0.996773\n",
      "Train: epoch: 41 loss: 0.943880 | Acc: 0.994981 | mAP: 0.000000\n",
      "Test: epoch: 41 loss: 0.399411 | Acc: 0.997490 | mAP: 0.000000\n",
      "Best epoch: 41 | Best acc: 0.997490\n",
      "Train: epoch: 42 loss: 0.729472 | Acc: 0.996116 | mAP: 0.000000\n",
      "Test: epoch: 42 loss: 0.387673 | Acc: 0.997490 | mAP: 0.000000\n",
      "Best epoch: 42 | Best acc: 0.997490\n",
      "Train: epoch: 43 loss: 0.607493 | Acc: 0.996056 | mAP: 0.000000\n",
      "Test: epoch: 43 loss: 0.587705 | Acc: 0.997132 | mAP: 0.000000\n",
      "Best epoch: 42 | Best acc: 0.997490\n",
      "Train: epoch: 44 loss: 0.773131 | Acc: 0.995459 | mAP: 0.000000\n",
      "Test: epoch: 44 loss: 0.501633 | Acc: 0.997610 | mAP: 0.000000\n",
      "Best epoch: 44 | Best acc: 0.997610\n",
      "Train: epoch: 45 loss: 0.692206 | Acc: 0.996116 | mAP: 0.000000\n",
      "Test: epoch: 45 loss: 0.394747 | Acc: 0.997490 | mAP: 0.000000\n",
      "Best epoch: 44 | Best acc: 0.997610\n",
      "Train: epoch: 46 loss: 0.703764 | Acc: 0.996236 | mAP: 0.000000\n",
      "Test: epoch: 46 loss: 0.304937 | Acc: 0.998148 | mAP: 0.000000\n",
      "Best epoch: 46 | Best acc: 0.998148\n",
      "Train: epoch: 47 loss: 0.696394 | Acc: 0.996714 | mAP: 0.000000\n",
      "Test: epoch: 47 loss: 0.497915 | Acc: 0.997670 | mAP: 0.000000\n",
      "Best epoch: 46 | Best acc: 0.998148\n",
      "Train: epoch: 48 loss: 0.468378 | Acc: 0.997550 | mAP: 0.000000\n",
      "Test: epoch: 48 loss: 0.327924 | Acc: 0.997968 | mAP: 0.000000\n",
      "Best epoch: 46 | Best acc: 0.998148\n",
      "Train: epoch: 49 loss: 0.487242 | Acc: 0.997192 | mAP: 0.000000\n",
      "Test: epoch: 49 loss: 0.265843 | Acc: 0.998387 | mAP: 0.000000\n",
      "Best epoch: 49 | Best acc: 0.998387\n",
      "Train: epoch: 50 loss: 0.632867 | Acc: 0.996415 | mAP: 0.000000\n",
      "Test: epoch: 50 loss: 0.443886 | Acc: 0.997789 | mAP: 0.000000\n",
      "Best epoch: 49 | Best acc: 0.998387\n",
      "Train: epoch: 51 loss: 0.568846 | Acc: 0.997192 | mAP: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: epoch: 51 loss: 0.222030 | Acc: 0.998805 | mAP: 0.000000\n",
      "Best epoch: 51 | Best acc: 0.998805\n",
      "Train: epoch: 52 loss: 0.655877 | Acc: 0.997371 | mAP: 0.000000\n",
      "Test: epoch: 52 loss: 0.333580 | Acc: 0.998446 | mAP: 0.000000\n",
      "Best epoch: 51 | Best acc: 0.998805\n",
      "Train: epoch: 53 loss: 1.123773 | Acc: 0.995698 | mAP: 0.000000\n",
      "Test: epoch: 53 loss: 0.972235 | Acc: 0.997909 | mAP: 0.000000\n",
      "Best epoch: 51 | Best acc: 0.998805\n",
      "Train: epoch: 54 loss: 0.764901 | Acc: 0.997311 | mAP: 0.000000\n",
      "Test: epoch: 54 loss: 0.215033 | Acc: 0.998865 | mAP: 0.000000\n",
      "Best epoch: 54 | Best acc: 0.998865\n",
      "Train: epoch: 55 loss: 0.446330 | Acc: 0.997670 | mAP: 0.000000\n",
      "Test: epoch: 55 loss: 0.226822 | Acc: 0.998924 | mAP: 0.000000\n",
      "Best epoch: 55 | Best acc: 0.998924\n",
      "Train: epoch: 56 loss: 1.082994 | Acc: 0.997012 | mAP: 0.000000\n",
      "Test: epoch: 56 loss: 0.221004 | Acc: 0.998805 | mAP: 0.000000\n",
      "Best epoch: 55 | Best acc: 0.998924\n",
      "Train: epoch: 57 loss: 0.449468 | Acc: 0.998088 | mAP: 0.000000\n",
      "Test: epoch: 57 loss: 0.407893 | Acc: 0.998805 | mAP: 0.000000\n",
      "Best epoch: 55 | Best acc: 0.998924\n",
      "Train: epoch: 58 loss: 0.430887 | Acc: 0.997550 | mAP: 0.000000\n",
      "Test: epoch: 58 loss: 0.209014 | Acc: 0.998865 | mAP: 0.000000\n",
      "Best epoch: 55 | Best acc: 0.998924\n",
      "Train: epoch: 59 loss: 0.589295 | Acc: 0.997132 | mAP: 0.000000\n",
      "Test: epoch: 59 loss: 0.177720 | Acc: 0.999223 | mAP: 0.000000\n",
      "Best epoch: 59 | Best acc: 0.999223\n",
      "Train: epoch: 60 loss: 0.336326 | Acc: 0.998267 | mAP: 0.000000\n",
      "Test: epoch: 60 loss: 0.233038 | Acc: 0.998984 | mAP: 0.000000\n",
      "Best epoch: 59 | Best acc: 0.999223\n",
      "Train: epoch: 61 loss: 0.553470 | Acc: 0.997311 | mAP: 0.000000\n",
      "Test: epoch: 61 loss: 0.197746 | Acc: 0.999104 | mAP: 0.000000\n",
      "Best epoch: 59 | Best acc: 0.999223\n",
      "Train: epoch: 62 loss: 0.574313 | Acc: 0.997490 | mAP: 0.000000\n",
      "Test: epoch: 62 loss: 0.198726 | Acc: 0.999104 | mAP: 0.000000\n",
      "Best epoch: 59 | Best acc: 0.999223\n",
      "Train: epoch: 63 loss: 0.328951 | Acc: 0.998088 | mAP: 0.000000\n",
      "Test: epoch: 63 loss: 0.219821 | Acc: 0.999104 | mAP: 0.000000\n",
      "Best epoch: 59 | Best acc: 0.999223\n",
      "Train: epoch: 64 loss: 0.562931 | Acc: 0.997729 | mAP: 0.000000\n",
      "Test: epoch: 64 loss: 0.208224 | Acc: 0.999104 | mAP: 0.000000\n",
      "Best epoch: 59 | Best acc: 0.999223\n",
      "Train: epoch: 65 loss: 0.505465 | Acc: 0.998207 | mAP: 0.000000\n",
      "Test: epoch: 65 loss: 0.137668 | Acc: 0.999223 | mAP: 0.000000\n",
      "Best epoch: 65 | Best acc: 0.999223\n",
      "Train: epoch: 66 loss: 0.375939 | Acc: 0.997849 | mAP: 0.000000\n",
      "Test: epoch: 66 loss: 0.702708 | Acc: 0.997789 | mAP: 0.000000\n",
      "Best epoch: 65 | Best acc: 0.999223\n",
      "Train: epoch: 67 loss: 0.788107 | Acc: 0.997012 | mAP: 0.000000\n",
      "Test: epoch: 67 loss: 0.171001 | Acc: 0.999223 | mAP: 0.000000\n",
      "Best epoch: 67 | Best acc: 0.999223\n",
      "Train: epoch: 68 loss: 0.327327 | Acc: 0.998626 | mAP: 0.000000\n",
      "Test: epoch: 68 loss: 0.191642 | Acc: 0.999044 | mAP: 0.000000\n",
      "Best epoch: 67 | Best acc: 0.999223\n",
      "Train: epoch: 69 loss: 0.398908 | Acc: 0.997968 | mAP: 0.000000\n",
      "Test: epoch: 69 loss: 0.107776 | Acc: 0.999402 | mAP: 0.000000\n",
      "Best epoch: 69 | Best acc: 0.999402\n",
      "Train: epoch: 70 loss: 0.371712 | Acc: 0.998387 | mAP: 0.000000\n",
      "Test: epoch: 70 loss: 0.140645 | Acc: 0.999343 | mAP: 0.000000\n",
      "Best epoch: 69 | Best acc: 0.999402\n",
      "Train: epoch: 71 loss: 0.247074 | Acc: 0.998685 | mAP: 0.000000\n",
      "Test: epoch: 71 loss: 0.111464 | Acc: 0.999402 | mAP: 0.000000\n",
      "Best epoch: 71 | Best acc: 0.999402\n",
      "Train: epoch: 72 loss: 0.182921 | Acc: 0.998924 | mAP: 0.000000\n",
      "Test: epoch: 72 loss: 0.074489 | Acc: 0.999582 | mAP: 0.000000\n",
      "Best epoch: 72 | Best acc: 0.999582\n",
      "Train: epoch: 73 loss: 0.269371 | Acc: 0.998865 | mAP: 0.000000\n",
      "Test: epoch: 73 loss: 0.097547 | Acc: 0.999402 | mAP: 0.000000\n",
      "Best epoch: 72 | Best acc: 0.999582\n",
      "Train: epoch: 74 loss: 0.292816 | Acc: 0.998506 | mAP: 0.000000\n",
      "Test: epoch: 74 loss: 0.067196 | Acc: 0.999582 | mAP: 0.000000\n",
      "Best epoch: 74 | Best acc: 0.999582\n",
      "Train: epoch: 75 loss: 0.320396 | Acc: 0.998506 | mAP: 0.000000\n",
      "Test: epoch: 75 loss: 0.070935 | Acc: 0.999462 | mAP: 0.000000\n",
      "Best epoch: 74 | Best acc: 0.999582\n",
      "Train: epoch: 76 loss: 0.191438 | Acc: 0.998924 | mAP: 0.000000\n",
      "Test: epoch: 76 loss: 0.242566 | Acc: 0.999163 | mAP: 0.000000\n",
      "Best epoch: 74 | Best acc: 0.999582\n",
      "Train: epoch: 77 loss: 0.203262 | Acc: 0.998984 | mAP: 0.000000\n",
      "Test: epoch: 77 loss: 0.130592 | Acc: 0.999223 | mAP: 0.000000\n",
      "Best epoch: 74 | Best acc: 0.999582\n",
      "Train: epoch: 78 loss: 0.233443 | Acc: 0.998865 | mAP: 0.000000\n",
      "Test: epoch: 78 loss: 0.036148 | Acc: 0.999880 | mAP: 0.000000\n",
      "Best epoch: 78 | Best acc: 0.999880\n",
      "Train: epoch: 79 loss: 0.493688 | Acc: 0.999163 | mAP: 0.000000\n",
      "Test: epoch: 79 loss: 0.256448 | Acc: 0.999283 | mAP: 0.000000\n",
      "Best epoch: 78 | Best acc: 0.999880\n",
      "Train: epoch: 80 loss: 0.462753 | Acc: 0.998626 | mAP: 0.000000\n",
      "Test: epoch: 80 loss: 0.623132 | Acc: 0.998626 | mAP: 0.000000\n",
      "Best epoch: 78 | Best acc: 0.999880\n",
      "Train: epoch: 81 loss: 0.489501 | Acc: 0.998446 | mAP: 0.000000\n",
      "Test: epoch: 81 loss: 0.034669 | Acc: 0.999880 | mAP: 0.000000\n",
      "Best epoch: 81 | Best acc: 0.999880\n",
      "Train: epoch: 82 loss: 0.359243 | Acc: 0.998028 | mAP: 0.000000\n",
      "Test: epoch: 82 loss: 0.154141 | Acc: 0.999163 | mAP: 0.000000\n",
      "Best epoch: 81 | Best acc: 0.999880\n",
      "Train: epoch: 83 loss: 0.165460 | Acc: 0.998984 | mAP: 0.000000\n",
      "Test: epoch: 83 loss: 0.038289 | Acc: 0.999880 | mAP: 0.000000\n",
      "Best epoch: 83 | Best acc: 0.999880\n",
      "Train: epoch: 84 loss: 0.182240 | Acc: 0.999104 | mAP: 0.000000\n",
      "Test: epoch: 84 loss: 0.042662 | Acc: 0.999880 | mAP: 0.000000\n",
      "Best epoch: 84 | Best acc: 0.999880\n",
      "Train: epoch: 85 loss: 0.090639 | Acc: 0.999343 | mAP: 0.000000\n",
      "Test: epoch: 85 loss: 0.032718 | Acc: 0.999880 | mAP: 0.000000\n",
      "Best epoch: 85 | Best acc: 0.999880\n",
      "Train: epoch: 86 loss: 0.132312 | Acc: 0.999462 | mAP: 0.000000\n",
      "Test: epoch: 86 loss: 0.020758 | Acc: 0.999940 | mAP: 0.000000\n",
      "Best epoch: 86 | Best acc: 0.999940\n",
      "Train: epoch: 87 loss: 0.091455 | Acc: 0.999402 | mAP: 0.000000\n",
      "Test: epoch: 87 loss: 0.064749 | Acc: 0.999641 | mAP: 0.000000\n",
      "Best epoch: 86 | Best acc: 0.999940\n",
      "Train: epoch: 88 loss: 0.606276 | Acc: 0.999163 | mAP: 0.000000\n",
      "Test: epoch: 88 loss: 0.036346 | Acc: 0.999821 | mAP: 0.000000\n",
      "Best epoch: 86 | Best acc: 0.999940\n",
      "Train: epoch: 89 loss: 0.084939 | Acc: 0.999462 | mAP: 0.000000\n",
      "Test: epoch: 89 loss: 0.044937 | Acc: 0.999761 | mAP: 0.000000\n",
      "Best epoch: 86 | Best acc: 0.999940\n",
      "Train: epoch: 90 loss: 0.200535 | Acc: 0.999104 | mAP: 0.000000\n",
      "Test: epoch: 90 loss: 0.147068 | Acc: 0.999522 | mAP: 0.000000\n",
      "Best epoch: 86 | Best acc: 0.999940\n",
      "Train: epoch: 91 loss: 0.337520 | Acc: 0.999044 | mAP: 0.000000\n",
      "Test: epoch: 91 loss: 0.031350 | Acc: 0.999880 | mAP: 0.000000\n",
      "Best epoch: 86 | Best acc: 0.999940\n",
      "Train: epoch: 92 loss: 0.164934 | Acc: 0.999343 | mAP: 0.000000\n",
      "Test: epoch: 92 loss: 0.007253 | Acc: 0.999940 | mAP: 0.000000\n",
      "Best epoch: 92 | Best acc: 0.999940\n",
      "Train: epoch: 93 loss: 0.113617 | Acc: 0.999343 | mAP: 0.000000\n",
      "Test: epoch: 93 loss: 0.001961 | Acc: 1.000000 | mAP: 0.000000\n",
      "Best epoch: 93 | Best acc: 1.000000\n",
      "Train: epoch: 94 loss: 0.191551 | Acc: 0.999223 | mAP: 0.000000\n",
      "Test: epoch: 94 loss: 0.001328 | Acc: 1.000000 | mAP: 0.000000\n",
      "Best epoch: 94 | Best acc: 1.000000\n",
      "Train: epoch: 95 loss: 0.132780 | Acc: 0.999522 | mAP: 0.000000\n",
      "Test: epoch: 95 loss: 0.022811 | Acc: 0.999880 | mAP: 0.000000\n",
      "Best epoch: 94 | Best acc: 1.000000\n",
      "Train: epoch: 96 loss: 0.068017 | Acc: 0.999582 | mAP: 0.000000\n",
      "Test: epoch: 96 loss: 0.024988 | Acc: 0.999940 | mAP: 0.000000\n",
      "Best epoch: 94 | Best acc: 1.000000\n",
      "Train: epoch: 97 loss: 0.182655 | Acc: 0.999223 | mAP: 0.000000\n",
      "Test: epoch: 97 loss: 0.003204 | Acc: 1.000000 | mAP: 0.000000\n",
      "Best epoch: 97 | Best acc: 1.000000\n",
      "Train: epoch: 98 loss: 0.054446 | Acc: 0.999701 | mAP: 0.000000\n",
      "Test: epoch: 98 loss: 0.001657 | Acc: 1.000000 | mAP: 0.000000\n",
      "Best epoch: 98 | Best acc: 1.000000\n",
      "Train: epoch: 99 loss: 0.077373 | Acc: 0.999701 | mAP: 0.000000\n",
      "Test: epoch: 99 loss: 0.000625 | Acc: 1.000000 | mAP: 0.000000\n",
      "Best epoch: 99 | Best acc: 1.000000\n",
      "Train: epoch: 100 loss: 0.115226 | Acc: 0.999402 | mAP: 0.000000\n",
      "Test: epoch: 100 loss: 0.457966 | Acc: 0.998685 | mAP: 0.000000\n",
      "Best epoch: 99 | Best acc: 1.000000\n",
      "Train: epoch: 101 loss: 0.272601 | Acc: 0.999163 | mAP: 0.000000\n",
      "Test: epoch: 101 loss: 0.057354 | Acc: 0.999821 | mAP: 0.000000\n",
      "Best epoch: 99 | Best acc: 1.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: epoch: 102 loss: 0.180317 | Acc: 0.999223 | mAP: 0.000000\n",
      "Test: epoch: 102 loss: 0.126083 | Acc: 0.999701 | mAP: 0.000000\n",
      "Best epoch: 99 | Best acc: 1.000000\n",
      "Train: epoch: 103 loss: 0.119196 | Acc: 0.999641 | mAP: 0.000000\n",
      "Test: epoch: 103 loss: 0.000475 | Acc: 1.000000 | mAP: 0.000000\n",
      "Best epoch: 103 | Best acc: 1.000000\n",
      "Train: epoch: 104 loss: 0.202668 | Acc: 0.999522 | mAP: 0.000000\n",
      "Test: epoch: 104 loss: 0.060300 | Acc: 0.999641 | mAP: 0.000000\n",
      "Best epoch: 103 | Best acc: 1.000000\n",
      "Train: epoch: 105 loss: 0.108226 | Acc: 0.999283 | mAP: 0.000000\n",
      "Test: epoch: 105 loss: 0.001481 | Acc: 1.000000 | mAP: 0.000000\n",
      "Best epoch: 105 | Best acc: 1.000000\n",
      "Train: epoch: 106 loss: 0.077028 | Acc: 0.999641 | mAP: 0.000000\n",
      "Test: epoch: 106 loss: 0.002415 | Acc: 1.000000 | mAP: 0.000000\n",
      "Best epoch: 106 | Best acc: 1.000000\n",
      "Train: epoch: 107 loss: 0.466938 | Acc: 0.999283 | mAP: 0.000000\n",
      "Test: epoch: 107 loss: 0.000986 | Acc: 1.000000 | mAP: 0.000000\n",
      "Best epoch: 107 | Best acc: 1.000000\n",
      "Train: epoch: 108 loss: 0.232820 | Acc: 0.999163 | mAP: 0.000000\n",
      "Test: epoch: 108 loss: 0.034782 | Acc: 0.999880 | mAP: 0.000000\n",
      "Best epoch: 107 | Best acc: 1.000000\n",
      "Train: epoch: 109 loss: 0.244224 | Acc: 0.999163 | mAP: 0.000000\n",
      "Test: epoch: 109 loss: 0.003259 | Acc: 1.000000 | mAP: 0.000000\n",
      "Best epoch: 109 | Best acc: 1.000000\n",
      "Train: epoch: 110 loss: 0.113533 | Acc: 0.999223 | mAP: 0.000000\n",
      "Test: epoch: 110 loss: 0.003358 | Acc: 1.000000 | mAP: 0.000000\n",
      "Best epoch: 110 | Best acc: 1.000000\n",
      "Train: epoch: 111 loss: 0.402821 | Acc: 0.998566 | mAP: 0.000000\n",
      "Test: epoch: 111 loss: 0.000200 | Acc: 1.000000 | mAP: 0.000000\n",
      "Best epoch: 111 | Best acc: 1.000000\n",
      "Train: epoch: 112 loss: 0.142775 | Acc: 0.999582 | mAP: 0.000000\n",
      "Test: epoch: 112 loss: 0.030706 | Acc: 0.999940 | mAP: 0.000000\n",
      "Best epoch: 111 | Best acc: 1.000000\n",
      "Train: epoch: 113 loss: 0.192141 | Acc: 0.999462 | mAP: 0.000000\n",
      "Test: epoch: 113 loss: 0.489361 | Acc: 0.999223 | mAP: 0.000000\n",
      "Best epoch: 111 | Best acc: 1.000000\n",
      "Train: epoch: 114 loss: 0.176285 | Acc: 0.999462 | mAP: 0.000000\n",
      "Test: epoch: 114 loss: 0.032076 | Acc: 0.999761 | mAP: 0.000000\n",
      "Best epoch: 111 | Best acc: 1.000000\n",
      "Train: epoch: 115 loss: 0.190704 | Acc: 0.999343 | mAP: 0.000000\n",
      "Test: epoch: 115 loss: 0.036802 | Acc: 0.999940 | mAP: 0.000000\n",
      "Best epoch: 111 | Best acc: 1.000000\n",
      "Train: epoch: 116 loss: 0.122616 | Acc: 0.999522 | mAP: 0.000000\n",
      "Test: epoch: 116 loss: 0.001395 | Acc: 1.000000 | mAP: 0.000000\n",
      "Best epoch: 116 | Best acc: 1.000000\n",
      "Train: epoch: 117 loss: 0.102985 | Acc: 0.999582 | mAP: 0.000000\n",
      "Test: epoch: 117 loss: 0.042120 | Acc: 0.999701 | mAP: 0.000000\n",
      "Best epoch: 116 | Best acc: 1.000000\n",
      "Train: epoch: 118 loss: 0.064272 | Acc: 0.999880 | mAP: 0.000000\n",
      "Test: epoch: 118 loss: 0.013033 | Acc: 0.999880 | mAP: 0.000000\n",
      "Best epoch: 116 | Best acc: 1.000000\n",
      "Train: epoch: 119 loss: 0.241251 | Acc: 0.999104 | mAP: 0.000000\n",
      "Test: epoch: 119 loss: 0.000361 | Acc: 1.000000 | mAP: 0.000000\n",
      "Best epoch: 119 | Best acc: 1.000000\n",
      "Train: epoch: 120 loss: 0.033795 | Acc: 0.999880 | mAP: 0.000000\n",
      "Test: epoch: 120 loss: 0.021593 | Acc: 0.999940 | mAP: 0.000000\n",
      "Best epoch: 119 | Best acc: 1.000000\n",
      "Train: epoch: 121 loss: 0.031652 | Acc: 0.999701 | mAP: 0.000000\n",
      "Test: epoch: 121 loss: 0.032319 | Acc: 0.999940 | mAP: 0.000000\n",
      "Best epoch: 119 | Best acc: 1.000000\n",
      "Train: epoch: 122 loss: 0.140227 | Acc: 0.999761 | mAP: 0.000000\n",
      "Test: epoch: 122 loss: 0.000145 | Acc: 1.000000 | mAP: 0.000000\n",
      "Best epoch: 122 | Best acc: 1.000000\n",
      "Train: epoch: 123 loss: 0.008288 | Acc: 0.999880 | mAP: 0.000000\n",
      "Test: epoch: 123 loss: 0.001372 | Acc: 1.000000 | mAP: 0.000000\n",
      "Best epoch: 123 | Best acc: 1.000000\n",
      "Train: epoch: 124 loss: 0.227229 | Acc: 0.998984 | mAP: 0.000000\n",
      "Test: epoch: 124 loss: 0.000386 | Acc: 1.000000 | mAP: 0.000000\n",
      "Best epoch: 124 | Best acc: 1.000000\n",
      "Train: epoch: 125 loss: 0.113838 | Acc: 0.999462 | mAP: 0.000000\n",
      "Test: epoch: 125 loss: 0.000164 | Acc: 1.000000 | mAP: 0.000000\n",
      "Best epoch: 125 | Best acc: 1.000000\n",
      "Train: epoch: 126 loss: 0.028699 | Acc: 0.999821 | mAP: 0.000000\n",
      "Test: epoch: 126 loss: 0.000115 | Acc: 1.000000 | mAP: 0.000000\n",
      "Best epoch: 126 | Best acc: 1.000000\n",
      "Train: epoch: 127 loss: 0.031274 | Acc: 0.999880 | mAP: 0.000000\n",
      "Test: epoch: 127 loss: 0.084832 | Acc: 0.999582 | mAP: 0.000000\n",
      "Best epoch: 126 | Best acc: 1.000000\n",
      "Train: epoch: 128 loss: 0.275370 | Acc: 0.999582 | mAP: 0.000000\n",
      "Test: epoch: 128 loss: 0.000068 | Acc: 1.000000 | mAP: 0.000000\n",
      "Best epoch: 128 | Best acc: 1.000000\n",
      "Train: epoch: 129 loss: 0.014592 | Acc: 0.999880 | mAP: 0.000000\n",
      "Test: epoch: 129 loss: 0.000047 | Acc: 1.000000 | mAP: 0.000000\n",
      "Best epoch: 129 | Best acc: 1.000000\n",
      "Train: epoch: 130 loss: 0.263766 | Acc: 0.999641 | mAP: 0.000000\n",
      "Test: epoch: 130 loss: 0.001612 | Acc: 1.000000 | mAP: 0.000000\n",
      "Best epoch: 130 | Best acc: 1.000000\n",
      "Train: epoch: 131 loss: 0.052390 | Acc: 0.999522 | mAP: 0.000000\n",
      "Test: epoch: 131 loss: 0.000043 | Acc: 1.000000 | mAP: 0.000000\n",
      "Best epoch: 131 | Best acc: 1.000000\n",
      "Train: epoch: 132 loss: 0.143507 | Acc: 0.999701 | mAP: 0.000000\n",
      "Test: epoch: 132 loss: 0.000039 | Acc: 1.000000 | mAP: 0.000000\n",
      "Best epoch: 132 | Best acc: 1.000000\n",
      "Train: epoch: 133 loss: 0.232461 | Acc: 0.999402 | mAP: 0.000000\n",
      "Test: epoch: 133 loss: 0.107512 | Acc: 0.999761 | mAP: 0.000000\n",
      "Best epoch: 132 | Best acc: 1.000000\n",
      "Train: epoch: 134 loss: 0.135813 | Acc: 0.999223 | mAP: 0.000000\n",
      "Test: epoch: 134 loss: 0.021968 | Acc: 0.999940 | mAP: 0.000000\n",
      "Best epoch: 132 | Best acc: 1.000000\n",
      "Train: epoch: 135 loss: 0.052194 | Acc: 0.999761 | mAP: 0.000000\n",
      "Test: epoch: 135 loss: 0.000085 | Acc: 1.000000 | mAP: 0.000000\n",
      "Best epoch: 135 | Best acc: 1.000000\n",
      "Train: epoch: 136 loss: 0.058331 | Acc: 0.999701 | mAP: 0.000000\n",
      "Test: epoch: 136 loss: 0.000302 | Acc: 1.000000 | mAP: 0.000000\n",
      "Best epoch: 136 | Best acc: 1.000000\n",
      "Train: epoch: 137 loss: 0.074687 | Acc: 0.999522 | mAP: 0.000000\n",
      "Test: epoch: 137 loss: 0.000140 | Acc: 1.000000 | mAP: 0.000000\n",
      "Best epoch: 137 | Best acc: 1.000000\n",
      "Train: epoch: 138 loss: 0.070209 | Acc: 0.999641 | mAP: 0.000000\n",
      "Test: epoch: 138 loss: 0.000075 | Acc: 1.000000 | mAP: 0.000000\n",
      "Best epoch: 138 | Best acc: 1.000000\n",
      "Train: epoch: 139 loss: 0.095557 | Acc: 0.999582 | mAP: 0.000000\n",
      "Test: epoch: 139 loss: 0.000045 | Acc: 1.000000 | mAP: 0.000000\n",
      "Best epoch: 139 | Best acc: 1.000000\n",
      "Train: epoch: 140 loss: 0.133879 | Acc: 0.999402 | mAP: 0.000000\n",
      "Test: epoch: 140 loss: 0.000074 | Acc: 1.000000 | mAP: 0.000000\n",
      "Best epoch: 140 | Best acc: 1.000000\n",
      "Train: epoch: 141 loss: 0.064115 | Acc: 0.999821 | mAP: 0.000000\n",
      "Test: epoch: 141 loss: 0.066673 | Acc: 0.999761 | mAP: 0.000000\n",
      "Best epoch: 140 | Best acc: 1.000000\n",
      "Train: epoch: 142 loss: 0.131519 | Acc: 0.999641 | mAP: 0.000000\n",
      "Test: epoch: 142 loss: 0.000524 | Acc: 1.000000 | mAP: 0.000000\n",
      "Best epoch: 142 | Best acc: 1.000000\n",
      "Train: epoch: 143 loss: 0.069943 | Acc: 0.999582 | mAP: 0.000000\n",
      "Test: epoch: 143 loss: 0.000548 | Acc: 1.000000 | mAP: 0.000000\n",
      "Best epoch: 143 | Best acc: 1.000000\n",
      "Train: epoch: 144 loss: 0.348961 | Acc: 0.998984 | mAP: 0.000000\n",
      "Test: epoch: 144 loss: 0.017668 | Acc: 0.999940 | mAP: 0.000000\n",
      "Best epoch: 143 | Best acc: 1.000000\n",
      "Train: epoch: 145 loss: 0.174341 | Acc: 0.999402 | mAP: 0.000000\n",
      "Test: epoch: 145 loss: 0.000295 | Acc: 1.000000 | mAP: 0.000000\n",
      "Best epoch: 145 | Best acc: 1.000000\n",
      "Train: epoch: 146 loss: 0.007037 | Acc: 0.999940 | mAP: 0.000000\n",
      "Test: epoch: 146 loss: 0.000061 | Acc: 1.000000 | mAP: 0.000000\n",
      "Best epoch: 146 | Best acc: 1.000000\n",
      "Train: epoch: 147 loss: 0.021560 | Acc: 0.999880 | mAP: 0.000000\n",
      "Test: epoch: 147 loss: 0.000035 | Acc: 1.000000 | mAP: 0.000000\n",
      "Best epoch: 147 | Best acc: 1.000000\n",
      "Train: epoch: 148 loss: 0.049353 | Acc: 0.999821 | mAP: 0.000000\n",
      "Test: epoch: 148 loss: 0.240221 | Acc: 0.999522 | mAP: 0.000000\n",
      "Best epoch: 147 | Best acc: 1.000000\n",
      "Train: epoch: 149 loss: 0.097610 | Acc: 0.999761 | mAP: 0.000000\n",
      "Test: epoch: 149 loss: 0.004511 | Acc: 0.999940 | mAP: 0.000000\n",
      "Best epoch: 147 | Best acc: 1.000000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader    \n",
    "from transformers import BertTokenizer\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "\n",
    "def seed_everything(seed=27):\n",
    "    '''\n",
    "    Set random seed for reproducible experiments\n",
    "    Inputs: seed number \n",
    "    '''\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "     \n",
    "    # Set random seed\n",
    "    seed_everything()  \n",
    "    \n",
    "    # Device Count\n",
    "    num_gpu = torch.cuda.device_count()\n",
    "    \n",
    "    # hyperparameters\n",
    "    bs = 20\n",
    "    epochs = 150\n",
    "    lr = 0.00001\n",
    "    \n",
    "    checkpoint_dir = 'checkpoints/v5/complex/'\n",
    "    \n",
    "    # train and test dataloader\n",
    "    train_seq = [2]#, 3, 4, 6, 7, 9, 10, 11, 12, 14, 15]\n",
    "    val_seq = [1, 5, 16]\n",
    "    folder_head = 'dataset/instruments18/seq_'\n",
    "    folder_tail = '/vqa/complex/*.txt'\n",
    "\n",
    "    labels = ['kidney',\n",
    "          'Idle', 'Grasping', 'Retraction', 'Tissue_Manipulation',\n",
    "          'Tool_Manipulation', 'Cutting', 'Cauterization', 'Suction', \n",
    "          'Looping', 'Suturing', 'Clipping', 'Staple', 'Ultrasound_Sensing',\n",
    "          'left-top', 'right-top', 'left-bottom', 'right-bottom']\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "                transforms.Resize((300,256)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "                ])\n",
    "\n",
    "    # train_dataset\n",
    "    train_dataset = SurgicalSentenceVQADataset(train_seq, folder_head, folder_tail, transform=transform)\n",
    "    train_dataloader = DataLoader(dataset=train_dataset, batch_size= bs, shuffle=True)\n",
    "\n",
    "    # Val_dataset\n",
    "    val_dataset = SurgicalSentenceVQADataset(val_seq, folder_head, folder_tail, transform=transform)\n",
    "    val_dataloader = DataLoader(dataset=val_dataset, batch_size= bs, shuffle=False)\n",
    "    \n",
    "    # model\n",
    "    model = Surgical_VQA(num_classes=len(labels)).cuda()\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    best_epoch = [0]\n",
    "    best_results = [0.0]\n",
    "    \n",
    "    for epoch in range(1, epochs):\n",
    "        train_model(epoch, model, train_dataloader, lr, tokenizer )\n",
    "        test_acc, test_c_acc, mAP = test_model(epoch, model, train_dataloader, tokenizer )\n",
    "    \n",
    "        if test_acc >= best_results[0]:\n",
    "            best_results[0] = test_acc\n",
    "            best_epoch[0] = epoch\n",
    "        \n",
    "        print('Best epoch: %d | Best acc: %.6f' %(best_epoch[0], best_results[0]))\n",
    "        checkpoint = {'lr': lr, 'b_s': bs, 'state_dict': model.state_dict() }\n",
    "        save_name = \"checkpoint_\" + str(epoch) + '_epoch.pth'\n",
    "        \n",
    "        torch.save(checkpoint, os.path.join(checkpoint_dir, save_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881a9e87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1063cf6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf09f13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86fc831",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
