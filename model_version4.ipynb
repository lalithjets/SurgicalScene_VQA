{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f0ee54a",
   "metadata": {},
   "source": [
    "# GPT-2 & ResNet50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264c834b",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c8cbe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "class SurgicalVQADataset(Dataset):\n",
    "    def __init__(self, seq, folder_head, folder_tail, labels, transform=None):\n",
    "        \n",
    "        self.transform = transform\n",
    "        \n",
    "        # files, question and answers\n",
    "        filenames = []\n",
    "        for curr_seq in seq: filenames = filenames + glob.glob(folder_head + str(curr_seq) + folder_tail)\n",
    "        self.vqas = []\n",
    "        for file in filenames:\n",
    "            file_data = open(file, \"r\")\n",
    "            lines = [line.strip(\"\\n\") for line in file_data if line != \"\\n\"]\n",
    "            file_data.close()\n",
    "            for line in lines: self.vqas.append([file, line])\n",
    "        print('Total files: %d | Total question: %.d' %(len(filenames), len(self.vqas)))\n",
    "        \n",
    "        # Labels\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.vqas)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # img\n",
    "        loc = self.vqas[idx][0].split('/')\n",
    "        img_loc = os.path.join(loc[0],loc[1],loc[2], 'left_frames',loc[-1].split('_')[0]+'.png')\n",
    "        img = Image.open(img_loc)\n",
    "        if self.transform: img = self.transform(img)\n",
    "            \n",
    "        # question and answer\n",
    "        question = self.vqas[idx][1].split('|')[0]\n",
    "        label = self.labels.index(str(self.vqas[idx][1].split('|')[1]))\n",
    "\n",
    "        return img, question, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a6e890",
   "metadata": {},
   "source": [
    "## Model Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d710b349",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "class Surgical_VQA(nn.Module):\n",
    "    def __init__(self, num_classes=12):\n",
    "        super(Surgical_VQA, self).__init__()\n",
    "\n",
    "        # text processing\n",
    "        self.text_feature_extractor = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "        # image processing\n",
    "        self.img_feature_extractor = models.resnet50(pretrained=True)\n",
    "        new_fc = nn.Sequential(*list(self.img_feature_extractor.fc.children())[:-1])\n",
    "        self.img_feature_extractor.fc = new_fc\n",
    "\n",
    "        #classifier\n",
    "        self.classifier = nn.Linear(2816, num_classes)\n",
    "\n",
    "    def forward(self, img, text):\n",
    "        img_feature = self.img_feature_extractor(img)\n",
    "        \n",
    "        text_feature = self.text_feature_extractor.encode(text)\n",
    "        text_feature = torch.tensor(text_feature).cuda()\n",
    "        \n",
    "        img_text_features = torch.cat((img_feature, text_feature), dim=1)\n",
    "        \n",
    "        out = self.classifier(img_text_features)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a85e42",
   "metadata": {},
   "source": [
    "## Model Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63597038",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "class Surgical_VQA(nn.Module):\n",
    "    def __init__(self, num_classes=12):\n",
    "        super(Surgical_VQA, self).__init__()\n",
    "\n",
    "        # text processing\n",
    "        self.text_feature_extractor = SentenceTransformer('bert-large-nli-mean-tokens')\n",
    "        # image processing\n",
    "        self.img_feature_extractor = models.resnet50(pretrained=True)\n",
    "        new_fc = nn.Sequential(*list(self.img_feature_extractor.fc.children())[:-1])\n",
    "        self.img_feature_extractor.fc = new_fc\n",
    "\n",
    "        #classifier\n",
    "        self.classifier = nn.Linear(3072, num_classes)\n",
    "\n",
    "    def forward(self, img, text):\n",
    "        img_feature = self.img_feature_extractor(img)\n",
    "        \n",
    "        text_feature = self.text_feature_extractor.encode(text)\n",
    "        text_feature = torch.tensor(text_feature).cuda()\n",
    "        \n",
    "        img_text_features = torch.cat((img_feature, text_feature), dim=1)\n",
    "        \n",
    "        out = self.classifier(img_text_features)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d0dabd",
   "metadata": {},
   "source": [
    "## Model Version 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "539b53ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/mobarak/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from InferSent.models import InferSent\n",
    "\n",
    "class Surgical_VQA(nn.Module):\n",
    "    def __init__(self, num_classes=12):\n",
    "        super(Surgical_VQA, self).__init__()\n",
    "\n",
    "        # text processing\n",
    "        params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
    "                        'pool_type': 'max', 'dpout_model': 0.0, 'version': 2}\n",
    "        self.text_feature_extractor = InferSent(params_model)\n",
    "        self.text_feature_extractor.load_state_dict(torch.load('InferSent/encoder/infersent2.pkl'))\n",
    "        self.text_feature_extractor.set_w2v_path('InferSent/fastText/crawl-300d-2M.vec')\n",
    "        self.text_feature_extractor.build_vocab_k_words(K=100000)\n",
    "        \n",
    "        # image processing\n",
    "        self.img_feature_extractor = models.resnet50(pretrained=True)\n",
    "        new_fc = nn.Sequential(*list(self.img_feature_extractor.fc.children())[:-1])\n",
    "        self.img_feature_extractor.fc = new_fc\n",
    "\n",
    "        #classifier\n",
    "        self.classifier = nn.Linear(6144, num_classes)\n",
    "\n",
    "    def forward(self, img, text):\n",
    "        img_feature = self.img_feature_extractor(img)\n",
    "        \n",
    "        text_feature = self.text_feature_extractor.encode(text) #infersent.encode(query)[0]\n",
    "        text_feature = torch.tensor(text_feature).cuda()\n",
    "        \n",
    "        img_text_features = torch.cat((img_feature, text_feature), dim=1)\n",
    "        \n",
    "        out = self.classifier(img_text_features)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82faa234",
   "metadata": {},
   "source": [
    "## Model Version 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f85edbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "\n",
    "\n",
    "class Surgical_VQA(nn.Module):\n",
    "    def __init__(self, num_classes=12):\n",
    "        super(Surgical_VQA, self).__init__()\n",
    "\n",
    "        # text processing\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.text_feature_extractor = GPT2Model.from_pretrained('gpt2')\n",
    " \n",
    "        # image processing\n",
    "        self.img_feature_extractor = models.resnet50(pretrained=True)\n",
    "        new_fc = nn.Sequential(*list(self.img_feature_extractor.fc.children())[:-1])\n",
    "        self.img_feature_extractor.fc = new_fc\n",
    "\n",
    "        #classifier\n",
    "        self.classifier = nn.Linear(2816, num_classes)\n",
    "\n",
    "    def forward(self, img, text):\n",
    "        \n",
    "        # image\n",
    "        img_feature = self.img_feature_extractor(img)\n",
    "        \n",
    "        # text\n",
    "        encoded_text = self.tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        encoded_text['input_ids'] = encoded_text['input_ids'].cuda()\n",
    "        encoded_text['attention_mask'] = encoded_text['attention_mask'].cuda()\n",
    "        text_feature = self.text_feature_extractor(**encoded_text)\n",
    "        text_feature = text_feature.last_hidden_state.swapaxes(1,2)\n",
    "        text_feature = F.adaptive_avg_pool1d(text_feature,1)\n",
    "        text_feature = text_feature.swapaxes(1,2).squeeze(1)        \n",
    "        img_text_features = torch.cat((img_feature, text_feature), dim=1)\n",
    "        \n",
    "        out = self.classifier(img_text_features)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd2e3b3",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8eef33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "def calc_acc(y_true, y_pred):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    return acc\n",
    "\n",
    "def calc_classwise_acc(y_true, y_pred):\n",
    "    matrix = confusion_matrix(y_true, y_pred)\n",
    "    classwise_acc = matrix.diagonal()/matrix.sum(axis=1)\n",
    "    return classwise_acc\n",
    "\n",
    "def calc_map(y_true, y_scores):\n",
    "    mAP = average_precision_score(y_true, y_scores,average=None)\n",
    "    return mAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23248629",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f25b694e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def test_model(epoch, model, valid_dataloader):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0.0    \n",
    "    label_true = None\n",
    "    label_pred = None\n",
    "    label_score = None\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (imgs, q, labels) in enumerate(valid_dataloader, 0):\n",
    "            questions = []\n",
    "            for question in q: questions.append(question)\n",
    "            imgs, labels = imgs.cuda(), labels.cuda()\n",
    "            \n",
    "            outputs = model(imgs, questions)\n",
    "\n",
    "            loss = criterion(outputs,labels)\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "            scores, predicted = torch.max(F.softmax(outputs, dim=1).data, 1)    \n",
    "            label_true = labels.data.cpu() if label_true == None else torch.cat((label_true, labels.data.cpu()), 0)\n",
    "            label_pred = predicted.data.cpu() if label_pred == None else torch.cat((label_pred, predicted.data.cpu()), 0)\n",
    "            label_score = scores.data.cpu() if label_score == None else torch.cat((label_score, scores.data.cpu()), 0)\n",
    "\n",
    "            \n",
    "    acc, c_acc, mAP = calc_acc(label_true, label_pred), calc_classwise_acc(label_true, label_pred), 0.0#calc_map(label_true, label_score)\n",
    "\n",
    "    print('Test: epoch: %d loss: %.6f | Acc: %.6f | mAP: %.6f' %(epoch, total_loss, acc, mAP))\n",
    "    print(c_acc)\n",
    "    \n",
    "    return (acc, c_acc, mAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c11ee3",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dccb841",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "def train_model(epoch, model, train_dataloader, lr):  # train model\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0.0    \n",
    "    label_true = None\n",
    "    label_pred = None\n",
    "    label_score = None\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr = lr, weight_decay = 0)\n",
    "    \n",
    "    for i, (imgs, q, labels) in enumerate(train_dataloader,0):\n",
    "        questions = []\n",
    "        for question in q: questions.append(question)\n",
    "        imgs, labels = imgs.cuda(), labels.cuda()\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(imgs, questions)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        scores, predicted = torch.max(F.softmax(outputs, dim=1).data, 1)    \n",
    "        label_true = labels.data.cpu() if label_true == None else torch.cat((label_true, labels.data.cpu()), 0)\n",
    "        label_pred = predicted.data.cpu() if label_pred == None else torch.cat((label_pred, predicted.data.cpu()), 0)\n",
    "        label_score = scores.data.cpu() if label_score == None else torch.cat((label_score, scores.data.cpu()), 0)\n",
    "\n",
    "    \n",
    "    # loss and acc\n",
    "    acc, c_acc, mAP = calc_acc(label_true, label_pred), calc_classwise_acc(label_true, label_pred), 0.0#calc_map(label_true, label_score)\n",
    "\n",
    "    print('Train: epoch: %d loss: %.6f | Acc: %.6f | mAP: %.6f' %(epoch, total_loss, acc, mAP))\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f2d921",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4154d89f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 1560 | Total question: 9014\n",
      "Total files: 447 | Total question: 2769\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "\n",
    "def seed_everything(seed=27):\n",
    "    '''\n",
    "    Set random seed for reproducible experiments\n",
    "    Inputs: seed number \n",
    "    '''\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "     \n",
    "    # Set random seed\n",
    "    seed_everything()  \n",
    "    \n",
    "    # Device Count\n",
    "    num_gpu = torch.cuda.device_count()\n",
    "    \n",
    "    # hyperparameters\n",
    "    bs = 32\n",
    "    epochs = 150\n",
    "    lr = 0.00001\n",
    "    \n",
    "    checkpoint_dir = 'checkpoints/v4/simple/'\n",
    "    \n",
    "    # train and test dataloader\n",
    "    train_seq = [2, 3, 4, 6, 7, 9, 10, 11, 12, 14, 15]\n",
    "    val_seq = [1, 5, 16]\n",
    "    folder_head = 'dataset/instruments18/seq_'\n",
    "    folder_tail = '/vqa/simple/*.txt'\n",
    "\n",
    "    labels = ['kidney',\n",
    "          'Idle', 'Grasping', 'Retraction', 'Tissue_Manipulation',\n",
    "          'Tool_Manipulation', 'Cutting', 'Cauterization', 'Suction', \n",
    "          'Looping', 'Suturing', 'Clipping', 'Staple', 'Ultrasound_Sensing',\n",
    "          'left-top', 'right-top', 'left-bottom', 'right-bottom']\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "                transforms.Resize((300,256)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "                ])\n",
    "\n",
    "    # train_dataset\n",
    "    train_dataset = SurgicalVQADataset(train_seq, folder_head, folder_tail, labels, transform=transform)\n",
    "    train_dataloader = DataLoader(dataset=train_dataset, batch_size= bs, shuffle=True)\n",
    "\n",
    "    # Val_dataset\n",
    "    val_dataset = SurgicalVQADataset(val_seq, folder_head, folder_tail, labels, transform=transform)\n",
    "    val_dataloader = DataLoader(dataset=val_dataset, batch_size= bs, shuffle=False)\n",
    "    \n",
    "    # model\n",
    "    model = Surgical_VQA(num_classes=len(labels)).cuda()\n",
    "    \n",
    "    best_epoch = [0]\n",
    "    best_results = [0.0]\n",
    "    \n",
    "    for epoch in range(1, epochs):\n",
    "        train_model(epoch, model, train_dataloader, lr)\n",
    "        test_acc, test_c_acc, mAP = test_model(epoch, model, train_dataloader)\n",
    "    \n",
    "        if test_acc >= best_results[0]:\n",
    "            best_results[0] = test_acc\n",
    "            best_epoch[0] = epoch\n",
    "        \n",
    "        print('Best epoch: %d | Best acc: %.6f' %(best_epoch[0], best_results[0]))\n",
    "        checkpoint = {'lr': lr, 'b_s': bs, 'state_dict': model.state_dict() }\n",
    "        save_name = \"checkpoint_\" + str(epoch) + '_epoch.pth'\n",
    "        \n",
    "        torch.save(checkpoint, os.path.join(checkpoint_dir, save_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6779af06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1c3288",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
