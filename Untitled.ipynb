{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "900c109d",
   "metadata": {},
   "source": [
    "## Sentence Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a015dac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "text = \"What is the fastest car in the\"\n",
    "text_feature_extractor = SentenceTransformer('bert-large-nli-mean-tokens')\n",
    "encoder_sentence = text_feature_extractor.encode(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489e40d6",
   "metadata": {},
   "source": [
    "## GPT - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffbd3d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = GPT2Model.from_pretrained('gpt2').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9ae7a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   83, 21949,  1438,  5633, 50256, 50256, 50256, 50256, 50256],\n",
      "        [10919,   318,  1172,  1470,  2700,    79,  1804, 23748,    30],\n",
      "        [31373, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [ 4919,   345,  1804, 50256, 50256, 50256, 50256, 50256, 50256]])\n",
      "tensor([[1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 768])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = ['tissue name ?', 'what is prograph forcep doing hello?','hello','how you doing']\n",
    "inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "print(inputs['input_ids'])\n",
    "print(inputs['attention_mask'])\n",
    "\n",
    "inputs['input_ids'] = inputs['input_ids'].cuda()\n",
    "inputs['attention_mask'] = inputs['attention_mask'].cuda()\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    outputs1 = outputs.last_hidden_state.swapaxes(1,2)\n",
    "\n",
    "outputs1 = F.adaptive_avg_pool1d(outputs1,1)\n",
    "outputs1 = outputs1.swapaxes(1,2).squeeze(1)\n",
    "outputs1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f8d80f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12794cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c5888f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fe15b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d649b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "# Import required libraries\n",
    "import torch\n",
    "from pytorch_transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Encode a text inputs\n",
    "text = \"tissue name ?\"\n",
    "indexed_tokens = tokenizer.encode(text)\n",
    "print(indexed_tokens)\n",
    "\n",
    "text = \"what is prograph forcep doing ?\"\n",
    "indexed_tokens = tokenizer.encode(text)\n",
    "print(indexed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1505d2a8",
   "metadata": {},
   "source": [
    "## InferSent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ab44e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "import torch\n",
    "from InferSent.models import InferSent\n",
    "\n",
    "params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
    "                'pool_type': 'max', 'dpout_model': 0.0, 'version': 2}\n",
    "infersent = InferSent(params_model)\n",
    "infersent.load_state_dict(torch.load('InferSent/encoder/infersent2.pkl'))\n",
    "infersent.set_w2v_path('InferSent/fastText/crawl-300d-2M.vec')\n",
    "infersent.build_vocab_k_words(K=100000)\n",
    "\n",
    "query = ['I had pizza and pasta', 'I had pizza and pasta', 'I had pizza and pasta']\n",
    "query_vec =  infersent.encode(query)\n",
    "print(len(query_vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a01a31f",
   "metadata": {},
   "source": [
    "## Rapid evaluation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567ff582",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "def calc_acc(y_true, y_pred):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    return acc\n",
    "\n",
    "def calc_classwise_acc(y_true, y_pred):\n",
    "    matrix = confusion_matrix(y_true, y_pred)\n",
    "    classwise_acc = matrix.diagonal()/matrix.sum(axis=1)\n",
    "    return classwise_acc\n",
    "\n",
    "def calc_map(y_true, y_scores):\n",
    "    mAP = average_precision_score(y_true, y_scores,average=None)\n",
    "    return mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e89459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "class SurgicalVQADataset(Dataset):\n",
    "    def __init__(self, seq, folder_head, folder_tail, labels, transform=None):\n",
    "        \n",
    "        self.transform = transform\n",
    "        \n",
    "        # files, question and answers\n",
    "        filenames = []\n",
    "        for curr_seq in seq: filenames = filenames + glob.glob(folder_head + str(curr_seq) + folder_tail)\n",
    "        self.vqas = []\n",
    "        for file in filenames:\n",
    "            file_data = open(file, \"r\")\n",
    "            lines = [line.strip(\"\\n\") for line in file_data if line != \"\\n\"]\n",
    "            file_data.close()\n",
    "            for line in lines: self.vqas.append([file, line])\n",
    "        print('Total files: %d | Total question: %.d' %(len(filenames), len(self.vqas)))\n",
    "        \n",
    "        # Labels\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.vqas)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # img\n",
    "        loc = self.vqas[idx][0].split('/')\n",
    "        img_loc = os.path.join(loc[0],loc[1],loc[2], 'left_frames',loc[-1].split('_')[0]+'.png')\n",
    "        img = Image.open(img_loc)\n",
    "        if self.transform: img = self.transform(img)\n",
    "            \n",
    "        # question and answer\n",
    "        question = self.vqas[idx][1].split('|')[0]\n",
    "        label = self.labels.index(str(self.vqas[idx][1].split('|')[1]))\n",
    "\n",
    "        return img, question, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f13e0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "class Surgical_VQA(nn.Module):\n",
    "    def __init__(self, num_classes=12):\n",
    "        super(Surgical_VQA, self).__init__()\n",
    "\n",
    "        # text processing\n",
    "        self.text_feature_extractor = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "        # image processing\n",
    "        self.img_feature_extractor = models.resnet50(pretrained=True)\n",
    "        new_fc = nn.Sequential(*list(self.img_feature_extractor.fc.children())[:-1])\n",
    "        self.img_feature_extractor.fc = new_fc\n",
    "\n",
    "        #classifier\n",
    "        self.classifier = nn.Linear(2816, num_classes)\n",
    "\n",
    "    def forward(self, img, text):\n",
    "        img_feature = self.img_feature_extractor(img)\n",
    "        \n",
    "        text_feature = self.text_feature_extractor.encode(text)\n",
    "        text_feature = torch.tensor(text_feature).cuda()\n",
    "        \n",
    "        img_text_features = torch.cat((img_feature, text_feature), dim=1)\n",
    "        \n",
    "        out = self.classifier(img_text_features)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4851f365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def test_model(epoch, model, valid_dataloader):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0.0    \n",
    "    label_true = None\n",
    "    label_pred = None\n",
    "    label_score = None\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (imgs, q, labels) in enumerate(valid_dataloader, 0):\n",
    "            questions = []\n",
    "            for question in q: questions.append(question)\n",
    "            imgs, labels = imgs.cuda(), labels.cuda()\n",
    "            \n",
    "            outputs = model(imgs, questions)\n",
    "\n",
    "            loss = criterion(outputs,labels)\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "            scores, predicted = torch.max(F.softmax(outputs, dim=1).data, 1)    \n",
    "            label_true = labels.data.cpu() if label_true == None else torch.cat((label_true, labels.data.cpu()), 0)\n",
    "            label_pred = predicted.data.cpu() if label_pred == None else torch.cat((label_pred, predicted.data.cpu()), 0)\n",
    "            label_score = scores.data.cpu() if label_score == None else torch.cat((label_score, scores.data.cpu()), 0)\n",
    "\n",
    "            \n",
    "    acc, c_acc, mAP = calc_acc(label_true, label_pred), calc_classwise_acc(label_true, label_pred), 0.0#calc_map(label_true, label_score)\n",
    "\n",
    "    print('Test: epoch: %d loss: %.6f | Acc: %.6f | mAP: %.6f' %(epoch, total_loss, acc, mAP))\n",
    "    print(c_acc)\n",
    "    \n",
    "    return (acc, c_acc, mAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399aa15b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "def seed_everything(seed=27):\n",
    "    '''\n",
    "    Set random seed for reproducible experiments\n",
    "    Inputs: seed number \n",
    "    '''\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "     \n",
    "    # Set random seed\n",
    "    seed_everything()  \n",
    "    \n",
    "    # Device Count\n",
    "    num_gpu = torch.cuda.device_count()\n",
    "    \n",
    "    # hyperparameters\n",
    "    bs = 32\n",
    "    epochs = 1\n",
    "    lr = 0.00001\n",
    "    \n",
    "    checkpoint_dir = 'checkpoints/v1/simple/'\n",
    "    \n",
    "    # train and test dataloader\n",
    "    train_seq = [2, 3, 4, 6, 7, 9, 10, 11, 12, 14, 15]\n",
    "    val_seq = [1, 5, 16]\n",
    "    folder_head = 'dataset/instruments18/seq_'\n",
    "    folder_tail = '/vqa/simple/*.txt'\n",
    "\n",
    "    labels = ['kidney',\n",
    "          'Idle', 'Grasping', 'Retraction', 'Tissue_Manipulation',\n",
    "          'Tool_Manipulation', 'Cutting', 'Cauterization', 'Suction', \n",
    "          'Looping', 'Suturing', 'Clipping', 'Staple', 'Ultrasound_Sensing',\n",
    "          'left-top', 'right-top', 'left-bottom', 'right-bottom']\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "                transforms.Resize((300,256)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "                ])\n",
    "\n",
    "    # train_dataset\n",
    "    train_dataset = SurgicalVQADataset(train_seq, folder_head, folder_tail, labels, transform=transform)\n",
    "    train_dataloader = DataLoader(dataset=train_dataset, batch_size= bs, shuffle=True)\n",
    "\n",
    "    # Val_dataset\n",
    "    val_dataset = SurgicalVQADataset(val_seq, folder_head, folder_tail, labels, transform=transform)\n",
    "    val_dataloader = DataLoader(dataset=val_dataset, batch_size= bs, shuffle=False)\n",
    "    \n",
    "    # model\n",
    "    model = Surgical_VQA(num_classes=len(labels)).cuda()\n",
    "    \n",
    "    best_epoch = [0]\n",
    "    best_results = [0.0]\n",
    "    \n",
    "    # load pre-trained model\n",
    "    print('Loading pre-trained weights')\n",
    "    pretrained_model = torch.load(('checkpoints/v1/simple/checkpoint_238_epoch.pth'))\n",
    "    pretrained_model = pretrained_model['state_dict']\n",
    "    model.load_state_dict(pretrained_model)\n",
    "    \n",
    "    acc, c_acc, mAP = test_model(epochs, model, train_dataloader)\n",
    "    \n",
    "#     for epoch in range(1, epochs):\n",
    "#         train_model(epoch, model, train_dataloader, lr)\n",
    "#         test_acc = test_model(epoch, model, train_dataloader)\n",
    "    \n",
    "#         if test_acc >= best_results[0]:\n",
    "#             best_results[0] = test_acc\n",
    "#             best_epoch[0] = epoch\n",
    "        \n",
    "#         print('Best epoch: %d | Best acc: %.6f' %(best_epoch[0], best_results[0]))\n",
    "#         checkpoint = {'lr': lr, 'b_s': bs, 'state_dict': model.state_dict() }\n",
    "#         save_name = \"checkpoint_\" + str(epoch) + '_epoch.pth'\n",
    "        \n",
    "#         torch.save(checkpoint, os.path.join(checkpoint_dir, save_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce1fc7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80afca00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36e0cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb5264c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8e4e6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52009e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
