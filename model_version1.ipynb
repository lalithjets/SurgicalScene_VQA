{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e5c58bc",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ce1e350",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "class SurgicalVQADataset(Dataset):\n",
    "    def __init__(self, seq, folder_head, folder_tail, labels, transform=None):\n",
    "        \n",
    "        self.transform = transform\n",
    "        \n",
    "        # files, question and answers\n",
    "        filenames = []\n",
    "        for curr_seq in seq: filenames = filenames + glob.glob(folder_head + str(curr_seq) + folder_tail)\n",
    "        self.vqas = []\n",
    "        for file in filenames:\n",
    "            file_data = open(file, \"r\")\n",
    "            lines = [line.strip(\"\\n\") for line in file_data if line != \"\\n\"]\n",
    "            file_data.close()\n",
    "            for line in lines: self.vqas.append([file, line])\n",
    "        print('Total files: %d | Total question: %.d' %(len(filenames), len(self.vqas)))\n",
    "        \n",
    "        # Labels\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.vqas)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # img\n",
    "        loc = self.vqas[idx][0].split('/')\n",
    "        img_loc = os.path.join(loc[0],loc[1],loc[2], 'left_frames',loc[-1].split('_')[0]+'.png')\n",
    "        img = Image.open(img_loc)\n",
    "        if self.transform: img = self.transform(img)\n",
    "            \n",
    "        # question and answer\n",
    "        question = self.vqas[idx][1].split('|')[0]\n",
    "        label = self.labels.index(str(self.vqas[idx][1].split('|')[1]))\n",
    "\n",
    "        return img, question, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bed23f4",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e76baa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "class Surgical_VQA(nn.Module):\n",
    "    def __init__(self, num_classes=12):\n",
    "        super(Surgical_VQA, self).__init__()\n",
    "\n",
    "        # text processing\n",
    "        self.text_feature_extractor = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "        # image processing\n",
    "        self.img_feature_extractor = models.resnet50(pretrained=True)\n",
    "        new_fc = nn.Sequential(*list(self.img_feature_extractor.fc.children())[:-1])\n",
    "        self.img_feature_extractor.fc = new_fc\n",
    "\n",
    "        #classifier\n",
    "        self.classifier = nn.Linear(2816, num_classes)\n",
    "\n",
    "    def forward(self, img, text):\n",
    "        img_feature = self.img_feature_extractor(img)\n",
    "        \n",
    "        text_feature = self.text_feature_extractor.encode(text)\n",
    "        text_feature = torch.tensor(text_feature).cuda()\n",
    "        \n",
    "        img_text_features = torch.cat((img_feature, text_feature), dim=1)\n",
    "        \n",
    "        out = self.classifier(img_text_features)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b865e6",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59515d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(epoch, model, valid_dataloader):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    total = 0\n",
    "    correct = 0\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (imgs, q, labels) in enumerate(valid_dataloader, 0):\n",
    "            questions = []\n",
    "            for question in q: questions.append(question)\n",
    "            imgs, labels = imgs.cuda(), labels.cuda()\n",
    "            \n",
    "            outputs = model(imgs, questions)\n",
    "            \n",
    "            loss = criterion(outputs,labels)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    # loss and acc\n",
    "    total_loss = total_loss / len(valid_dataloader)\n",
    "    test_acc = (correct/total)\n",
    "    \n",
    "    print('Test: epoch: %d loss: %.6f | Acc: %.6f' %(epoch, total_loss, test_acc))\n",
    "    \n",
    "    return (test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53aa2351",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58391718",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "def train_model(epoch, model, train_dataloader, lr):  # train model\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    total = 0\n",
    "    correct = 0\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr = lr, weight_decay = 0)\n",
    "    \n",
    "    for i, (imgs, q, labels) in enumerate(train_dataloader,0):\n",
    "        questions = []\n",
    "        for question in q: questions.append(question)\n",
    "        imgs, labels = imgs.cuda(), labels.cuda()\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(imgs, questions)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    # loss and acc\n",
    "    total_loss = total_loss / len(train_dataloader)\n",
    "    train_acc = (correct/total)\n",
    "\n",
    "    print('Train: epoch: %d loss: %.6f | Acc: %.6f' %(epoch, total_loss, train_acc))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deac4ee0",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2799092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 1560 | Total question: 9014\n",
      "Total files: 447 | Total question: 2769\n",
      "Train: epoch: 1 loss: 2.195718 | Acc: 0.304859\n",
      "Test: epoch: 1 loss: 1.791958 | Acc: 0.493011\n",
      "Best epoch: 1 | Best acc: 0.493011\n",
      "Train: epoch: 2 loss: 1.667827 | Acc: 0.523297\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "def seed_everything(seed=27):\n",
    "    '''\n",
    "    Set random seed for reproducible experiments\n",
    "    Inputs: seed number \n",
    "    '''\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "     \n",
    "    # Set random seed\n",
    "    seed_everything()  \n",
    "    \n",
    "    # Device Count\n",
    "    num_gpu = torch.cuda.device_count()\n",
    "    \n",
    "    # hyperparameters\n",
    "    bs = 32\n",
    "    epochs = 250\n",
    "    lr = 0.00001\n",
    "    \n",
    "    checkpoint_dir = 'checkpoints/v1/simple/'\n",
    "    \n",
    "    # train and test dataloader\n",
    "    train_seq = [2, 3, 4, 6, 7, 9, 10, 11, 12, 14, 15]\n",
    "    val_seq = [1, 5, 16]\n",
    "    folder_head = 'dataset/instruments18/seq_'\n",
    "    folder_tail = '/vqa/simple/*.txt'\n",
    "\n",
    "    labels = ['kidney',\n",
    "          'Idle', 'Grasping', 'Retraction', 'Tissue_Manipulation',\n",
    "          'Tool_Manipulation', 'Cutting', 'Cauterization', 'Suction', \n",
    "          'Looping', 'Suturing', 'Clipping', 'Staple', 'Ultrasound_Sensing',\n",
    "          'left-top', 'right-top', 'left-bottom', 'right-bottom']\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "                transforms.Resize((300,256)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "                ])\n",
    "\n",
    "    # train_dataset\n",
    "    train_dataset = SurgicalVQADataset(train_seq, folder_head, folder_tail, labels, transform=transform)\n",
    "    train_dataloader = DataLoader(dataset=train_dataset, batch_size= bs, shuffle=True)\n",
    "\n",
    "    # Val_dataset\n",
    "    val_dataset = SurgicalVQADataset(val_seq, folder_head, folder_tail, labels, transform=transform)\n",
    "    val_dataloader = DataLoader(dataset=val_dataset, batch_size= bs, shuffle=False)\n",
    "    \n",
    "    # model\n",
    "    model = Surgical_VQA(num_classes=len(labels)).cuda()\n",
    "    \n",
    "    best_epoch = [0]\n",
    "    best_results = [0.0]\n",
    "    \n",
    "    for epoch in range(1, epochs):\n",
    "        train_model(epoch, model, train_dataloader, lr)\n",
    "        test_acc = test_model(epoch, model, train_dataloader)\n",
    "    \n",
    "        if test_acc >= best_results[0]:\n",
    "            best_results[0] = test_acc\n",
    "            best_epoch[0] = epoch\n",
    "        \n",
    "        print('Best epoch: %d | Best acc: %.6f' %(best_epoch[0], best_results[0]))\n",
    "        checkpoint = {'lr': lr, 'b_s': bs, 'state_dict': model.state_dict() }\n",
    "        save_name = \"checkpoint_\" + str(epoch) + '_epoch.pth'\n",
    "        \n",
    "        torch.save(checkpoint, os.path.join(checkpoint_dir, save_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f95d34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a010e4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
